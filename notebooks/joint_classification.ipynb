{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Joint classification\n",
    "\n",
    "Author: Edoardo De Matteis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torchaudio\n",
    "!pip install torchinfo\n",
    "!pip install pytorch_lightning\n",
    "!pip install wandb -qqq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, Union, List, Any\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Callback, seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor\n",
    ")\n",
    "from torchinfo import summary\n",
    "\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filesystem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd /content/gdrive/MyDrive/Colab\\ Notebooks/Birdcalls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_callbacks(callbacks: Dict) -> List[Callback]:\n",
    "    out_callbacks: List[Callback] = []\n",
    "    if \"lr_monitor\" in callbacks:\n",
    "        out_callbacks.append(\n",
    "            LearningRateMonitor(\n",
    "                logging_interval=callbacks[\"lr_monitor\"][\"logging_interval\"],\n",
    "                log_momentum=callbacks[\"lr_monitor\"][\"log_momentum\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if \"early_stopping\" in callbacks:\n",
    "        out_callbacks.append(\n",
    "            EarlyStopping(\n",
    "                monitor=callbacks[\"monitor_metric\"],\n",
    "                mode=callbacks[\"monitor_metric_mode\"],\n",
    "                patience=callbacks[\"early_stopping\"][\"patience\"],\n",
    "                verbose=callbacks[\"early_stopping\"][\"verbose\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if \"model_checkpoints\" in callbacks:\n",
    "        out_callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                monitor=callbacks[\"monitor_metric\"],\n",
    "                mode=callbacks[\"monitor_metric_mode\"],\n",
    "                save_top_k=callbacks[\"model_checkpoints\"][\"save_top_k\"],\n",
    "                verbose=callbacks[\"model_checkpoints\"][\"verbsose\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return out_callbacks\n",
    "\n",
    "def cnn_size(\n",
    "    input: Tuple[int, int],\n",
    "    kernel: Union[int, Tuple[int, int]],\n",
    "    padding: Union[int, Tuple[int, int]] = 0,\n",
    "    stride: Union[int, Tuple[int, int]] = 1,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Return the size of the output of a convolutional layer.\n",
    "    :param input: Size of the input image.\n",
    "    :param kernel: Kernel size, it is assumed to be a square.\n",
    "    :param padding: Padding size.\n",
    "    :param stride: Stride.\n",
    "    :return: The output size.\n",
    "    \"\"\"\n",
    "    if isinstance(kernel, int):\n",
    "        kernel = (kernel, kernel)\n",
    "\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)\n",
    "\n",
    "    out_w = (input[0] - kernel[0] + 2 * padding[0]) / stride[0] + 1\n",
    "    out_h = (input[1] - kernel[1] + 2 * padding[1]) / stride[1] + 1\n",
    "    return int(out_w), int(out_h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class JointDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Union[str, Path, None],\n",
    "        online: bool,\n",
    "        debug: int,\n",
    "        load: bool,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param csv_path: Path of the training CSV file.\n",
    "        :param online: If true tensors are computed on-the-fly by the dataloader, otherwise they are all precomputed.\n",
    "        :param debug: Defines the size of the reduced dataset (it is shuffled beforehand) we want to use, any number\n",
    "        below or equal to 0 means that we keep the whole dataset.\n",
    "        :param load: If true we do not compute anything and will load values from a file.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(JointDataset, self).__init__()\n",
    "\n",
    "        self.online = online\n",
    "        self.len: int\n",
    "\n",
    "        self.spectrograms: torch.Tensor\n",
    "        self.targets: torch.Tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def load(\n",
    "        spectrograms_path: Union[str, Path], targets_path: Union[str, Path], **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load a dataset whose spectorgrams and targets are loaded from .pt files.\n",
    "        :param spectrograms_path: Path of the spectrograms tensor file.\n",
    "        :param targets_path: Path of the targets tensor file.\n",
    "        :param kwargs:\n",
    "        :return: A SoundscapeDataset object with populated tensors.\n",
    "        \"\"\"\n",
    "        ds = JointDataset(csv_path=None, online=False, debug=-1, load=True)\n",
    "\n",
    "        ds.spectrograms = torch.load(spectrograms_path)\n",
    "        ds.targets = torch.load(targets_path)\n",
    "        ds.len = len(ds.targets)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        :param item: Index of the item to retrieve.\n",
    "        :return: The item-th entry.\n",
    "        \"\"\"\n",
    "        if self.online:\n",
    "            return {\n",
    "                \"row_id\": self.row_id[item],\n",
    "                \"site\": self.site[item],\n",
    "                \"audio_id\": self.audio_id[item],\n",
    "                \"seconds\": self.seconds[item],\n",
    "                \"birds\": self.birds[item],\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"spectrograms\": self.spectrograms[item],\n",
    "                \"targets\": self.targets[item],\n",
    "            }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Datamodule"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class JointDataModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_workers: Dict,\n",
    "        batch_size: Dict,\n",
    "        shuffle: Dict,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # These attributes will be populated after self.setup() call.\n",
    "        self.train_ds: Optional[Dataset] = None\n",
    "        self.val_ds: Optional[Dataset] = None\n",
    "        self.test_ds: Optional[Dataset] = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        if stage is None or stage == \"fit\":\n",
    "            # Train\n",
    "            self.train_ds = JointDataset.load(\n",
    "                spectrograms_path=TRAIN_JOINT_DEBUG_SPECTROGRAMS,\n",
    "                targets_path=TRAIN_JOINT_DEBUG_TARGETS\n",
    "            )\n",
    "\n",
    "            # Val\n",
    "            self.val_ds = JointDataset.load(\n",
    "                spectrograms_path=VAL_JOINT_DEBUG_SPECTROGRAMS,\n",
    "                targets_path=VAL_JOINT_DEBUG_TARGETS\n",
    "            )\n",
    "\n",
    "    def train_dataloader(\n",
    "        self,\n",
    "    ) -> Union[DataLoader, List[DataLoader], Dict[str, DataLoader]]:\n",
    "        batch_size = self.batch_size[\"train\"]\n",
    "        shuffle = self.shuffle[\"train\"]\n",
    "\n",
    "        dl = DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "\n",
    "        return dl\n",
    "\n",
    "    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n",
    "        batch_size = self.batch_size[\"val\"]\n",
    "        shuffle = self.shuffle[\"val\"]\n",
    "\n",
    "        dl = DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "\n",
    "        return dl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNNAtt(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_shape: Tuple[int, int, int],\n",
    "        channels: List,\n",
    "        kernels: List,\n",
    "        paddings: List,\n",
    "        strides: List,\n",
    "        num_heads: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A layer with some CNNs and attention.\n",
    "        :param image_shape: The shape of the input image (channels, width, height).\n",
    "        :param channels: The number of output channels for the CNNs, the last one has to be 1.\n",
    "        :param kernels: A tuple with kernels for first, second and third convolution.\n",
    "        :param paddings: A tuple with paddings for first, second and third convolution.\n",
    "        :param strides: A tuple with strides for first, second and third convolution.\n",
    "        :param num_heads: Number of heads to use in the attention layer, by default one to avoid prime numbers' issues.\n",
    "        \"\"\"\n",
    "        super(CNNAtt, self).__init__()\n",
    "        c, w, h = image_shape\n",
    "\n",
    "        # Are used three CNNs since is the minimum needed to have a bottleneck and return to the original channel size,\n",
    "        # yet is still possible to learn the identity function as a composition of f and its inverse.\n",
    "        channels_start, channels_mid, channels_end = 1, 3, 1\n",
    "        kernel_start, kernel_mid, kernel_end = kernels\n",
    "        padding_start, padding_mid, padding_end = paddings\n",
    "        stride_start, stride_mid, stride_end = strides\n",
    "\n",
    "        # Query\n",
    "        self.cnn_q = self.get_seq(\n",
    "            name=\"CNNQuery\",\n",
    "            channels=channels,\n",
    "            kernels=kernels,\n",
    "            paddings=paddings,\n",
    "            strides=strides,\n",
    "        )\n",
    "\n",
    "        # Key\n",
    "        self.cnn_k = self.get_seq(\n",
    "            name=\"CNNKey\",\n",
    "            channels=channels,\n",
    "            kernels=kernels,\n",
    "            paddings=paddings,\n",
    "            strides=strides,\n",
    "        )\n",
    "\n",
    "        # Value\n",
    "        self.cnn_v = self.get_seq(\n",
    "            name=\"CNNValue\",\n",
    "            channels=channels,\n",
    "            kernels=kernels,\n",
    "            paddings=paddings,\n",
    "            strides=strides,\n",
    "        )\n",
    "\n",
    "        # Measure output sizes.\n",
    "        seq, embed = self.count_outputs(\n",
    "            input=(w, h), kernels=kernels, paddings=paddings, strides=strides\n",
    "        )\n",
    "\n",
    "        # Attention layers, one for each image dimension.\n",
    "        self.att1 = nn.MultiheadAttention(\n",
    "            embed_dim=embed, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "        self.att2 = nn.MultiheadAttention(\n",
    "            embed_dim=seq, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        q = self.cnn_q(xb).squeeze()\n",
    "        k = self.cnn_k(xb).squeeze()\n",
    "        v = self.cnn_v(xb).squeeze()\n",
    "\n",
    "        # Transpose since batch first is usually faster, in att2 to swap row with columns.\n",
    "        att1, _ = self.att1(q, k, v)\n",
    "        att2, _ = self.att2(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2))\n",
    "\n",
    "        # Add one dimension and concatenate as a 2-channels image.\n",
    "        att1 = att1.unsqueeze(1)\n",
    "        att2 = att2.transpose(1, 2).unsqueeze(1)\n",
    "        out = torch.cat((att1, att2), dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_seq(\n",
    "        self,\n",
    "        name: str,\n",
    "        channels: List,\n",
    "        kernels: List,\n",
    "        paddings: List,\n",
    "        strides: List,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This function returns a list of modules that build up a basic block. Each one will be used as query, key or value\n",
    "        in an attention layer.\n",
    "        :param name: Name of the layer.\n",
    "        :param channels: Channels of each CNN layer, the last one has to be 1.\n",
    "        :param kernels: Kernel size for each CNN.\n",
    "        :param paddings: Paddings for each CNN.\n",
    "        :param strides: Strides for each CNN.\n",
    "        :return: A nn.Sequential object.\n",
    "        \"\"\"\n",
    "        assert channels[-1] == 1, \"The attention layer can accept one channel only!.\"\n",
    "\n",
    "        # By default audio data has only one channel.\n",
    "        in_channels = 1\n",
    "        seq = nn.Sequential()\n",
    "\n",
    "        for n, (out_channels, kernel, padding, stride) in enumerate(\n",
    "            zip(channels, kernels, paddings, strides)\n",
    "        ):\n",
    "            cnn = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "            )\n",
    "            seq.add_module(name=f\"{name}{n}\", module=cnn)\n",
    "            in_channels = out_channels\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def count_outputs(\n",
    "        self,\n",
    "        input: Tuple[int, int],\n",
    "        kernels: List,\n",
    "        paddings: List,\n",
    "        strides: List,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Just count the output of an image obtained by a sequence of convolutions.\n",
    "        :param input: Input image dimension.\n",
    "        :param kernels: List of kernels to apply.\n",
    "        :param paddings: List of paddings.\n",
    "        :param strides: List of strides.\n",
    "        :return: The size of the feature image.\n",
    "        \"\"\"\n",
    "        image = input\n",
    "        for kernel, padding, stride in zip(kernels, paddings, strides):\n",
    "            image = cnn_size(input=image, kernel=kernel, padding=padding, stride=stride)\n",
    "        return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CNNRes(nn.Module):\n",
    "    def __init__(self, in_channels: int, kernel_size: int):\n",
    "        super(CNNRes, self).__init__()\n",
    "\n",
    "        # This padding is added to keep dimensionality the same, it is recommended to choose even kernels.\n",
    "        pad = kernel_size // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding=pad,\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding=pad,\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding=pad,\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding=pad,\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding=pad,\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=(kernel_size, kernel_size),\n",
    "            padding=pad,\n",
    "        )\n",
    "\n",
    "    def forward(self, res):\n",
    "        out = self.conv1(res)\n",
    "        res = self.conv1(out) + res\n",
    "        out = self.conv1(res)\n",
    "        res = self.conv1(out) + res\n",
    "        out = self.conv1(res)\n",
    "        res = self.conv1(out)\n",
    "        return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Extraction(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extraction backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_shape: Tuple[int, int, int],\n",
    "        att_channels: List,\n",
    "        att_kernels: List,\n",
    "        att_paddings: List,\n",
    "        att_strides: List,\n",
    "        att_num_heads: int,\n",
    "        pool_att: int,\n",
    "        res_kernels: List,\n",
    "        pool: int,\n",
    "    ):\n",
    "        super(Extraction, self).__init__()\n",
    "        self.level = len(res_kernels)\n",
    "\n",
    "        # # CNNAtt blocks.\n",
    "        # self.cnnatt1 = CNNAtt(\n",
    "        #     image_shape=image_shape,\n",
    "        #     channels=att_channels,\n",
    "        #     kernels=att_kernels,\n",
    "        #     paddings=att_paddings,\n",
    "        #     strides=att_strides,\n",
    "        #     num_heads=att_num_heads,\n",
    "        # )\n",
    "\n",
    "        # self.cnnatt2 = CNNAtt(\n",
    "        #     image_shape=image_shape,\n",
    "        #     channels=att_channels,\n",
    "        #     kernels=att_kernels,\n",
    "        #     paddings=att_paddings,\n",
    "        #     strides=att_strides,\n",
    "        #     num_heads=att_num_heads,\n",
    "        # )\n",
    "\n",
    "        # # CNN for the concatenated attentions, it returns us an image of the same shape as the input and\n",
    "        # # can be used for residuals. The number of input channels is fixed to 4 due to it being 1*2*2:\n",
    "        # # - 1 the number of out channels in CNNAtt.\n",
    "        # # - 2 due to the concatenation of attentions in CNNAtt.\n",
    "        # # - 2 due to the concatenation of attentions self.forward().\n",
    "        # self.cnn_att = nn.Conv2d(in_channels=4, out_channels=1, kernel_size=(1, 1))\n",
    "\n",
    "        # # # Pooling before blocks, can be useful to lighten computation.\n",
    "        # self.pool_att = nn.AvgPool2d(kernel_size=pool_att)\n",
    "\n",
    "        # Residual blocks and convolutions in between them used to change dimensionality/filter sizes.\n",
    "        # We can fix out_channels since all audio data has originally one channel.\n",
    "        self.res = nn.Sequential()\n",
    "        in_channels = image_shape[0]\n",
    "\n",
    "        for n, kernel in enumerate(res_kernels):\n",
    "            res = CNNRes(in_channels=in_channels, kernel_size=kernel)\n",
    "            cnn = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=2 * in_channels,\n",
    "                kernel_size=kernel,\n",
    "                stride=(2, 2),\n",
    "            )\n",
    "\n",
    "            self.res.add_module(name=f\"CNNRes{n+1}\", module=res)\n",
    "            self.res.add_module(name=f\"CNN{n+1}\", module=cnn)\n",
    "\n",
    "            in_channels *= 2\n",
    "\n",
    "        # Output pooling.\n",
    "        self.pool = nn.AvgPool2d(kernel_size=pool)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # Get attentions.\n",
    "        # att1 = self.cnnatt1(xb)\n",
    "        # att2 = self.cnnatt1(xb)\n",
    "        # out = torch.cat((att1, att2), dim=1)\n",
    "\n",
    "        # # Conv to use residuals on.\n",
    "        # out = self.cnn_att(out) + xb\n",
    "        # out = self.pool_att(out)\n",
    "\n",
    "        # Residuals.\n",
    "        out = self.res(xb) # Change to out\n",
    "\n",
    "        # Pooling.\n",
    "        out = self.pool(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The architecture of the network is fixed to avoid having too clutter in the hyperparameters section."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, out_features: int, **kwargs):\n",
    "        super().__init__()\n",
    "        # self.ext = Extraction(\n",
    "        #     image_shape=(1, 128, 3751),\n",
    "        #     att_channels=[1, 3, 1],\n",
    "        #     att_kernels=[1, 3, 1],\n",
    "        #     att_paddings=[0, 1, 0],\n",
    "        #     att_strides=[1, 1, 1],\n",
    "        #     att_num_heads=1,\n",
    "        #     pool_att=1,\n",
    "        #     res_kernels=[3,5],\n",
    "        #     pool=1,\n",
    "        # )\n",
    "\n",
    "        # self.gru = nn.GRU(\n",
    "        #     input_size=112320,\n",
    "        #     hidden_size=512,\n",
    "        #     num_layers=1,\n",
    "        #     bidirectional=True,\n",
    "        #     dropout=0,  # Dropout should be 0 when there is only one layer.\n",
    "        # )\n",
    "\n",
    "        # ResNet18\n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1)\n",
    "        self.resnet = torchvision.models.resnet50(pretrained=True)\n",
    "        # for param in self.resnet.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=1000, out_features=out_features\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # # Feature extraction backbone.\n",
    "        # out = self.ext(xb)\n",
    "\n",
    "        # # Reshape.\n",
    "        # b, c, w, h = out.shape\n",
    "        # out = out.reshape(b, 1, c * w * h).transpose(0, 1)\n",
    "\n",
    "        # # Prediction head.\n",
    "        # # out, weights = self.att(out, out, out)\n",
    "        # out, _ = self.gru(out)\n",
    "        # logits = self.fc(out.squeeze())\n",
    "\n",
    "        out = self.cnn(xb)\n",
    "        out = self.resnet(out)\n",
    "        # print(\"SGRAOX\"*30)\n",
    "        # print(out.shape)\n",
    "        logits = self.fc(out)\n",
    "\n",
    "\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class JointClassification(pl.LightningModule):\n",
    "    def __init__(self, out_features: int, **kwargs):\n",
    "        super(JointClassification, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = Classification(out_features=out_features)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        metric = torchmetrics.Accuracy()\n",
    "        self.train_accuracy = metric.clone()\n",
    "        self.val_accuracy = metric.clone()\n",
    "        self.test_accuracy = metric.clone()\n",
    "\n",
    "    def forward(self, xb):\n",
    "        logits = self.model(xb)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        return logits, preds\n",
    "\n",
    "    def step(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        logits, preds = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        return {\"logits\": logits, \"preds\": preds, \"loss\": loss}\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "        targets = batch[\"targets\"]\n",
    "        specs = batch[\"spectrograms\"]\n",
    "        out_step = self.step(x=specs, y=targets)\n",
    "\n",
    "        self.train_accuracy(out_step[\"preds\"], targets)\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": out_step[\"loss\"], \"train_acc\": self.train_accuracy.compute()}\n",
    "        )\n",
    "        return out_step[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        targets = batch[\"targets\"]\n",
    "        specs = batch[\"spectrograms\"]\n",
    "        out_step = self.step(x=specs, y=targets)\n",
    "\n",
    "        self.val_accuracy(out_step[\"preds\"], targets)\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": out_step[\"loss\"], \"val_acc\": self.val_accuracy.compute()}\n",
    "        )\n",
    "        return out_step[\"loss\"]\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        opt = self.hparams.optim[\"optimizer\"][\"fn\"](\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams.optim[\"optimizer\"][\"lr\"],\n",
    "            betas=self.hparams.optim[\"optimizer\"][\"betas\"],\n",
    "            eps=self.hparams.optim[\"optimizer\"][\"eps\"],\n",
    "            weight_decay=self.hparams.optim[\"optimizer\"][\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "\n",
    "        if not self.hparams.optim[\"use_lr_scheduler\"]:\n",
    "            return {\"optimizer\": opt}\n",
    "        else:\n",
    "            scheduler = self.hparams.optim[\"lr_scheduler\"][\"fn\"](\n",
    "                optimizer=opt,\n",
    "                T_0=self.hparams.optim[\"lr_scheduler\"][\"T_0\"],\n",
    "                T_mult=self.hparams.optim[\"lr_scheduler\"][\"T_mult\"],\n",
    "                eta_min=self.hparams.optim[\"lr_scheduler\"][\"eta_min\"],\n",
    "                last_epoch=self.hparams.optim[\"lr_scheduler\"][\"last_epoch\"],\n",
    "                verbose=self.hparams.optim[\"lr_scheduler\"][\"verbose\"],\n",
    "            )\n",
    "            return {\"optimizer\": opt, \"lr_scheduler\": scheduler}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Environmental variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(\"/content/gdrive/My Drive/Colab Notebooks/Birdcalls\")\n",
    "\n",
    "TRAIN_BIRDCALLS_DEBUG = Path(\"/content/gdrive/My Drive/Colab Notebooks/Birdcalls/out/debug_datasets/train/birdcalls\")\n",
    "TRAIN_BIRDCALLS_DEBUG_SPECTROGRAMS= Path(\"/content/gdrive/My Drive/Colab Notebooks/Birdcalls/out/debug_datasets/train/joint/spectrograms.pt\")\n",
    "TRAIN_BIRDCALLS_DEBUG_TARGETS= Path(\"/content/gdrive/My Drive/Colab Notebooks/Birdcalls/out/debug_datasets/train/joint/targets.pt\")\n",
    "\n",
    "VAL_BIRDCALLS_DEBUG= Path(\"/content/gdrive/My Drive/Colab Notebooks/Birdcalls/out/debug_datasets/val/birdcalls\")\n",
    "VAL_BIRDCALLS_DEBUG_SPECTROGRAMS= Path(\"/content/gdrive/My Drive/Colab Notebooks/Birdcalls/out/debug_datasets/val/joint/spectrograms_balanced.pt\")\n",
    "VAL_BIRDCALLS_DEBUG_TARGETS= Path(\"/content/gdrive/My Drive/Colab Notebooks/Birdcalls/out/debug_datasets/val/joint/targets_balanced.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input dictionaries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_workers = {'train': 12, 'val': 12}\n",
    "batch_size = {'train': 8, 'val': 8}\n",
    "shuffle = {'train': True, 'val': False}\n",
    "\n",
    "# Optimizer\n",
    "optimizer = {'fn': torch.optim.Adam,\n",
    "             'lr': 1e-4,\n",
    "             'betas': [ 0.9, 0.999 ],\n",
    "             'eps': 1e-08,\n",
    "             'weight_decay': 0\n",
    "             }\n",
    "\n",
    "use_lr_scheduler = False\n",
    "\n",
    "lr_scheduler = {'fn': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "                'T_0': 10,\n",
    "                'T_mult': 2,\n",
    "                'eta_min': 0,\n",
    "                'last_epoch': -1,\n",
    "                'verbose': True}\n",
    "\n",
    "optim = {'optimizer': optimizer,\n",
    "         'use_lr_scheduler': use_lr_scheduler,\n",
    "         'lr_scheduler': lr_scheduler}\n",
    "\n",
    "# Callbacks\n",
    "lr_monitor = {\n",
    "    \"logging_interval\": \"step\",\n",
    "    \"log_momentum\": False\n",
    "}\n",
    "\n",
    "early_stopping = {\n",
    "    \"patience\": 42,\n",
    "    \"verbose\": False\n",
    "}\n",
    "\n",
    "model_checkpoints = {\n",
    "    \"save_top_k\": 2,\n",
    "    \"verbose\": False\n",
    "}\n",
    "\n",
    "callbacks = {\n",
    "    \"monitor_metric\": 'train_loss',\n",
    "    \"monitor_metric_mode\": 'min',\n",
    "    # \"lr_monitor\": lr_monitor,\n",
    "    # \"model_checkpoints\": model_checkpoints,\n",
    "    # \"early_stopping\": early_stopping\n",
    "}\n",
    "\n",
    "# Trainer\n",
    "train = {\n",
    "    \"deterministic\": True,\n",
    "    \"random_seed\": 42,\n",
    "    \"val_check_interval\": 1.0,\n",
    "    \"progress_bar_refresh_rate\": 20,\n",
    "    \"fast_dev_run\": True, # True for debug purposes.\n",
    "    \"gpus\": -1 if torch.cuda.is_available() else 0,\n",
    "    \"precision\": 32,\n",
    "    \"max_steps\": 100,\n",
    "    \"max_epochs\": 25,\n",
    "    \"accumulate_grad_batches\": 1,\n",
    "    \"num_sanity_val_steps\": 2,\n",
    "    \"gradient_clip_val\": 10.0\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Call that only once!\n",
    "if train[\"deterministic\"]:\n",
    "    seed_everything(train[\"random_seed\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "datamodule = JointDataModule(num_workers=num_workers,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=shuffle)\n",
    "\n",
    "model = JointClassification(optim=optim, out_features=397)\n",
    "# callbacks=build_callbacks(callbacks=callbacks),\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"Birdcalls classification\",\n",
    "    config={\n",
    "        \"batch_size\": batch_size['train'],\n",
    "        \"learning_rate\": optimizer['lr'],\n",
    "        \"optimizer\": optimizer['fn'],\n",
    "        \"betas\": optimizer[\"betas\"],\n",
    "        \"eps\": optimizer[\"eps\"],\n",
    "        \"weight_decay\": optimizer[\"weight_decay\"],\n",
    "        \"lr_scheduler\": use_lr_scheduler,\n",
    "        \"T_0\": lr_scheduler[\"T_0\"],\n",
    "        \"T_mult\": lr_scheduler[\"T_mult\"],\n",
    "        \"eta_min\": lr_scheduler[\"eta_min\"],\n",
    "        \"last_epoch\": lr_scheduler[\"last_epoch\"],\n",
    "        \"dataset\": \"Bird CLEF 2021\",\n",
    "        \"summary\": summary(model),\n",
    "        }\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        logger=wandb_logger,\n",
    "        deterministic=train[\"deterministic\"],\n",
    "        gpus=train[\"gpus\"],\n",
    "        max_epochs=train[\"max_epochs\"],\n",
    "        # callbacks=callbacks\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(summary(model))\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(model=model, datamodule=datamodule)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.validate(model=model, datamodule=datamodule)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quit W&B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}